<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Personal academic homepage">
  <meta name="author" content="Matt Graham">

  <title>Matt Graham</title>

  <!-- Bootstrap core CSS -->
  <link href="./css/bootstrap.css" rel="stylesheet">

  <!-- FontAwesomeCSS -->
  <link rel="stylesheet" href="./css/font-awesome.min.css">

  <!-- Academicons CSS -->
  <link rel="stylesheet" href="./css/academicons.min.css"/>

  <!-- Custom icons font CSS -->
  <link rel="stylesheet" href="./css/custom-icons.css"/>

  <!-- Bootstrap social css -->
  <link rel="stylesheet" href="./css/bootstrap-social.css">

  <!-- Site specific CSS -->
  <link href="./css/custom.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
  <![endif]-->

</head>

<body data-spy="scroll">

  <div class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="#">Matt Graham</a>
      </div>
      <div class="navbar-collapse collapse">
        <ul class="nav navbar-nav" id="mainNav">
          <li class="active"><a href="#intro">intro</a></li>
          <li><a href="#contact">contact</a></li>
          <li><a href="#publications">publications</a></li>
          <li><a href="#talks">talks</a></li>
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </div>

  <div class="container">

    <div class="row" id="intro">
      <div class="col-xs-12">
        <div class="row section-header">
          <h1>Introduction</h1>
        </div>
        <div class="row">
          <div class="col-md-3" id="profile-photo-and-social-col">
            <div class="text-center">
              <img src="images/profile-photo-mm-graham-small.jpg" class="img-thumbnail" width="200" />
            </div>
            <div class="text-center vertical-buffer">
              <div class="btn-group" role="group">
                <a href="https://github.com/matt-graham" class="btn btn-social-icon btn-github">
                  <i class="fa fa-github fa-fw"></i>
                </a>
                <a href="https://orcid.org/0000-0001-9104-7960" class="btn btn-social-icon btn-orcid-green">
                  <i class="ai ai-orcid fa-fw ai-3x"></i>
                </a>
                <a href="https://scholar.google.co.uk/citations?user=1QMZI4kAAAAJ&hl=en" class="btn btn-social-icon btn-google-yellow">
                  <i style="margin-top: 2px;" class="fa icon-google-scholar-g-thicker fa-fw"></i>
                </a>
                <a href="http://arxiv.org/a/graham_m_2" class="btn btn-social-icon btn-arxiv-red">
                  <i class="ai ai-arxiv fa-fw ai-3x"></i>
                </a>
              </div>
            </div>
          </div> <!--/.profile-photo-and-social-col-->
          <div class="col-md-9" id="intro-text-col">
            <p class="lead">I am a research data scientist in the <a href="https://www.ucl.ac.uk/advanced-research-computing/advanced-research-computing-centre">Advanced Research Computing Centre</a> at University College London. Prior to my current role, I did postdocs with <a href="https://oates.work/">Chris Oates</a> at the <a href="https://www.ncl.ac.uk/maths-physics/">School of Mathematics, Statistics and Physics</a> at Newcastle University and <a href="http://www.normalesup.org/~athiery/index.html">Alex Thiery</a> at the <a href="https://www.stat.nus.edu.sg/">Department of Statistics and Applied Probability</a> in the National University of Singapore. I completed my PhD at the <a href="https://www.ed.ac.uk/informatics">School of Informatics</a> in the University of Edinburgh, where I was supervised by <a href="https://homepages.inf.ed.ac.uk/amos/">Amos Storkey</a>.
            </p>
            <p class="lead">
              <span class="text-muted">Research interests:</span> Markov chain Monte Carlo methods, Hamiltonian Monte Carlo, data assimilation, inverse problems, probabilistic programming.
            </p>
          </div><!--./intro-text-col-->
        </div>
      </div>
    </div><!--./intro-->

    <div class="row" id="contact">
      <div class="col-xs-12">
        <div class="row section-header">
          <h1>Contact</h1>
        </div>
        <div class="row">
          <div class="col-md-4" id='contact-email-col'>
            <div class="panel panel-default">
              <div class="panel-heading"><i class="fa fa-envelope fa-fw"></i> Email</div>
              <div class="panel-body"><a href="mailto:m[dot]graham[at]ucl[dot]ac[dot]uk">m&#46;graham&#8203;&#64;&#8203;ucl&#46;ac&#46;uk</a></div>
            </div><!--./contact-email-col-->
          </div>
          <div class="col-md-8" id="contact-address-col">
            <div class="panel panel-default">
              <div class="panel-heading"><i class="fa fa-building fa-fw"></i> Address</div>
              <div class="panel-body">UCL Advanced Research Computing Centre, Bidborough House, London, WC1H 9BT.</div>
            </div>
          </div><!--./contact-address-col-->
        </div>
      </div>
    </div><!--./contact-->

    <div class="row" id="publications">
      <div class="col-xs-12">
        <div class="row section-header">
          <h1>Publications</h1>
        </div>
        <div class="row">
          <h2 class='sub-heading'>Pre-prints</h2>
          <ul class="publications-list">
            <li class="publication">
              <h4>
                <small>2023/03</small>
                ParticleDA.jl v.1.0: A real-time data assimilation software platform
              </h4>
              <p>
                <a href="https://www.ucl.ac.uk/statistics/daniel-giles">Daniel Giles</a>, <i>Matthew M. Graham</i>, <a href="https://giordano.github.io/">Mosè Giordano</a>, <a href="https://iris.ucl.ac.uk/iris/browse/profile?upi=TKOSK42">Tuomas Koskela</a>, <a href="http://www.homepages.ucl.ac.uk/~ucakabe/">Alexandros Beskos</a> and <a href="https://www.ucl.ac.uk/statistics/people/sergeguillas">Serge Guillas</a>
              </p>
              <div class="btn-group" role="group">
                <a href="https://gmd.copernicus.org/preprints/gmd-2023-38/" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Pre-print
                </a>
                <a href="https://github.com/Team-RADDISH/ParticleDA.jl" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#particleda-abs">
                  Abstract
                </button>
              </div>
              <div id="particleda-abs" class="collapse abstract">
                Digital twins of physical and human systems informed by real-time data, are becoming ubiquitous across weather forecasting, disaster preparedness, and urban planning, but researchers lack the tools to run these models effectively and efficiently, limiting progress. One of the current challenges is to assimilate observations in highly nonlinear dynamical systems, as the practical need is often to detect abrupt changes. We developed a software platform to improve the use of real-time data in highly nonlinear system representations where non-Gaussianity prevents the use of more standard Data Assimilation. Optimal Particle filtering data assimilation (DA) techniques have been implemented within an user-friendly open source software platform in Julia – ParticleDA.jl. To ensure the applicability of the developed platform in realistic scenarios, emphasis has been placed on numerical efficiency, scalability and optimisation for high performance computing frameworks. Furthermore, the platform has been developed to be forward model agnostic, ensuring that it is applicable to a wide range of modelling settings, for instance unstructured and non-uniform meshes in the spatial domain or even state spaces that are not spatially organised. Applications to tsunami and numerical weather prediction demonstrate the computational benefits in terms of lower errors, lower computational costs (due to ensemble size and the algorithm's overheads being minimised) and versatility thanks to flexible I/O in a high level language Julia.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2022/11</small>
                Parameter estimation with increased precision for elliptic and hypo-elliptic diffusions
              </h4>
              <p>
                <a href="https://iris.ucl.ac.uk/iris/browse/profile?upi=YIGUC90">Yuga Iguchi</a>, <a href="http://www.homepages.ucl.ac.uk/~ucakabe/">Alexandros Beskos</a> and <i>Matthew M. Graham</i>
              </p>
              <div class="btn-group" role="group">
                <a href="https://arxiv.org/abs/2211.16384" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> Pre-print
                </a>
                <a href="https://github.com/matt-graham/simsde" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#hypodiff-abs">
                  Abstract
                </button>
              </div>
              <div id="hypodiff-abs" class="collapse abstract">
                This work aims at making a comprehensive contribution in the general area of parametric inference for partially observed diffusion processes. Established approaches for likelihood-based estimation invoke a numerical time-discretisation scheme for the approximation of the (typically intractable) transition dynamics of the Stochastic Differential Equation (SDE) model over finite time periods. The scheme is applied for a step-size that is either a user-selected tuning parameter or determined by the data. Recent research has highlighted the critical effect of the choice of numerical scheme on the behaviour of derived parameter estimates in the setting of hypo-elliptic SDEs. In brief, in our work, first, we develop two weak second order `sampling schemes' (to cover both the hypo-elliptic and elliptic SDE classes) and generate accompanying `transition density schemes' of the SDE (i.e., approximations of the SDE transition density). Then, we produce a collection of analytic results, providing a complete theoretical framework that solidifies the proposed schemes and showcases advantages from their incorporation within SDE calibration methods. We present numerical results from carrying out classical or Bayesian inference, for both elliptic and hypo-elliptic SDE models.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2019/06</small>
                A scalable optimal-transport based local particle filter
              </h4>
              <p>
                <i>Matthew M. Graham</i> and <a href="http://www.normalesup.org/~athiery/index.html">Alexandre H. Thiery</a>
              </p>
              <div class="btn-group" role="group">
                <a href="https://arxiv.org/abs/1906.00507" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> Pre-print
                </a>
                <a href="files/sletpf-poster-isba.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Poster
                </a>
                <a href="slides/sletpf/nus.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://github.com/thiery-lab/data-assimilation" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#sletpf-abs">
                  Abstract
                </button>
              </div>
              <div id="sletpf-abs" class="collapse abstract">
                  Filtering in spatially-extended dynamical systems is a challenging problem with significant practical applications such as numerical weather prediction. Particle filters allow asymptotically consistent inference but require infeasibly large ensemble sizes for accurate estimates in complex spatial models. Localisation approaches, which perform local state updates by exploiting low dependence between variables at distant points, have been suggested as a potential resolution to this issue. Naively applying the resampling step of the particle filter locally however produces implausible spatially discontinuous states. The ensemble transform particle filter replaces resampling with an optimal-transport map and can be localised by computing maps for every spatial mesh node. The resulting local ensemble transport particle filter is however computationally intensive for dense meshes. We propose a new optimal-transport based local particle filter which computes a fixed number of maps independent of the mesh resolution and interpolates these maps across space, reducing the computation required and allowing it to be ensured particles remain spatially smooth. We numerically illustrate that, at a reduced computational cost, we are able to achieve the same accuracy as the local ensemble transport particle filter, and retain its improved robustness to non-Gaussianity and ability to quantify uncertainty when compared to local ensemble Kalman filters.
              </div>
            </li>
          </ul>
          <h2 class='sub-heading'> Journal articles</h2>
          <ul class="publications-list">
            <li class="publication">
              <h4>
                <small>2023/04
                </small>
                Manifold lifting: scaling Markov chain Monte Carlo to the vanishing noise regime
              </h4>
              <p>
                <a href="https://kx-au.github.io">Khai Xiang Au</a>, <i>Matthew M. Graham</i> and <a href="http://www.normalesup.org/~athiery/index.html">Alexandre H. Thiery</a>
              </p>
              <p class='published-in'>
                Journal of the Royal Statistical Society: Series B (Statistical Methodology)
              </p>
              <div class="btn-group" role="group">
                <a href="https://doi.org/10.1093/jrsssb/qkad023" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Article
                </a>
                <a href="https://arxiv.org/abs/2003.03950" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> Pre-print
                </a>
                <a href="slides/manifold-lifting" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://github.com/thiery-lab/manifold_lifting" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#manifold-lifting-abs">
                  Abstract
                </button>
              </div>
              <div id="manifold-lifting-abs" class="collapse abstract">
                Standard Markov chain Monte Carlo methods struggle to explore distributions that concentrate in the neighbourhood of low-dimensional submanifolds. This pathology naturally occurs in Bayesian inference settings when there is a high signal-to-noise ratio in the observational data but the model is inherently over-parametrised or nonidentifiable. In this paper, we propose a strategy that transforms the original sampling problem into the task of exploring a distribution supported on a manifold embedded in a higher-dimensional space; in contrast to the original posterior this lifted distribution remains diffuse in the limit of vanishing observation noise. We employ a constrained Hamiltonian Monte Carlo method, which exploits the geometry of this lifted distribution, to perform efficient approximate inference. We demonstrate in numerical experiments that, contrarily to competing approaches, the sampling efficiency of our proposed methodology does not degenerate as the target distribution to be explored concentrates near low-dimensional submanifolds.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2022/08</small>
                Testing whether a learning procedure is calibrated
              </h4>
              <p>
                <a href="http://www.joncockayne.com/">Jon Cockayne</a>, <i>Matthew M. Graham</i>, <a href="https://oates.work/">Chris Oates</a>, <a href="http://www.tjsullivan.org.uk//">Tim J. Sullivan</a> and <a href="http://teymur.uk/">Onyur Teymur</a>
              </p>
              <p class='published-in'>
                Journal of Machine Learning Research
              </p>
              <div class="btn-group" role="group">
                <a href="https://jmlr.org/papers/v23/21-1065.html"  class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Article
                </a>              
                <a href="https://arxiv.org/abs/2012.12670" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> Pre-print
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#calibration-abs">
                  Abstract
                </button>
              </div>
              <div id="calibration-abs" class="collapse abstract">
                A learning procedure takes as input a dataset and performs inference for the parameters θ of a model that is assumed to have given rise to the dataset. Here we consider learning procedures whose output is a probability distribution, representing uncertainty about θ after seeing the dataset. Bayesian inference is a prime example of such a procedure, but one can also construct other learning procedures that return distributional output. This paper studies conditions for a learning procedure to be considered calibrated, in the sense that the true data-generating parameters are plausible as samples from its distributional output. A learning procedure whose inferences and predictions are systematically over- or under-confident will fail to be calibrated. On the other hand, a learning procedure that is calibrated need not be statistically efficient. A hypothesis-testing framework is developed in order to assess, using simulation, whether a learning procedure is calibrated. Several vignettes are presented to illustrate different aspects of the framework
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2022/04</small>
                Manifold Markov chain Monte Carlo methods for Bayesian inference in diffusion models
              </h4>
              <p>
                <i>Matthew M. Graham</i>, <a href="http://www.normalesup.org/~athiery/index.html">Alexandre H. Thiery</a> and <a href="http://www.homepages.ucl.ac.uk/~ucakabe/">Alexandros Beskos</a>
              </p>
              <p class='published-in'>
                Journal of the Royal Statistical Society: Series B (Statistical Methodology)
              </p>
              <div class="btn-group" role="group">
                <a href="https://doi.org/10.1111/rssb.12497" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Article
                </a>
                <a href="https://arxiv.org/abs/1912.02982" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> Pre-print
                </a>
                <a href="slides/sde" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://github.com/thiery-lab/manifold-mcmc-for-diffusions" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#sde-mcmc-abs">
                  Abstract
                </button>
              </div>
              <div id="sde-mcmc-abs" class="collapse abstract">
                  Bayesian inference for nonlinear diffusions, observed at discrete times, is a challenging task that has prompted the development of a number of algorithms, mainly within the computational statistics community. We propose a new direction, and accompanying methodology—borrowing ideas from statistical physics and computational chemistry—for inferring the posterior distribution of latent diffusion paths and model parameters, given observations of the process. Joint configurations of the underlying process noise and of parameters, mapping onto diffusion paths consistent with observations, form an implicitly defined manifold. Then, by making use of a constrained Hamiltonian Monte Carlo algorithm on the embedded manifold, we are able to perform computationally efficient inference for a class of discretely observed diffusion models. Critically, in contrast with other approaches proposed in the literature, our methodology is highly automated, requiring minimal user intervention and applying alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. Exploiting Markovianity, we propose a variant of the method with complexity that scales linearly in the resolution of path discretisation and the number of observation times. Python code reproducing the results is available at http://doi.org/10.5281/zenodo.5796148.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2017/12</small>
                Asymptotically exact inference in differentiable generative models
              </h4>
              <p>
                <i>Matthew M. Graham</i> and <a href="http://homepages.inf.ed.ac.uk/amos">Amos J. Storkey</a>
              </p>
              <p class='published-in'>
                Electronic Journal of Statistics
              </p>
              <div class="btn-group" role="group">
                <a href="http://dx.doi.org/10.1214/17-EJS1340SI" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Article
                </a>
                <a href="files/dgm-poster-bayescomp.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Poster
                </a>
                <a href="slides/dgm/nus.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://github.com/matt-graham/differentiable-generative-models" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#dgm-ejs-abs">
                  Abstract
                </button>
              </div>
              <div id="dgm-ejs-abs" class="collapse abstract">
                Many generative models can be expressed as a differentiable function applied to input variables sampled from a known probability distribution. This framework includes both the generative component of learned parametric models such as variational autoencoders and generative adversarial networks, and also procedurally defined simulator models which involve only differentiable operations. Though the distribution on the input variables to such models is known, often the distribution on the output variables is only implicitly defined. We present a method for performing efficient Markov chain Monte Carlo inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where approximate Bayesian computation might otherwise be employed. We use the intuition that computing conditional expectations is equivalent to integrating over a density defined on the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to move between inputs exactly consistent with observations. We validate the method by performing inference experiments in a diverse set of models.
              </div>
            </li>
          </ul>
          <h2 class='sub-heading'>Conference proceedings</h2>
          <ul class="publications-list">
          <li class="publication">
            <h4>
              <small>2021/04</small>
              Measure transport with kernel Stein discrepancy
            </h4>
            <p>
              <a href="https://www.ncl.ac.uk/bigdata/people/people/fishermatthew.html">Matthew A. Fisher</a>, <a href="https://www.uts.edu.au/staff/tui.nolan">Tui Nolan</a>, <i>Matthew M. Graham</i>, <a href="https://dennisprangle.github.io/">Dennis Prangle</a> and <a href="https://oates.work/">Chris Oates</a>
            </p>
            <p class='published-in'>
              Proceedings of the 24th International Conference on Artificial Intelligence and Statistics
            </p>
            <div class="btn-group" role="group">
              <a href="http://proceedings.mlr.press/v130/fisher21a.html" class="btn btn-default">
                <i class="fa fa-file-o fa-fw"></i> Proceedings
              </a>
              <a href="https://arxiv.org/abs/2010.11779" class="btn btn-default">
                <i class="ai ai-arxiv fa-fw"></i> arXiv
              </a>
              <a href="https://github.com/MatthewAlexanderFisher/MTKSD" class="btn btn-default">
                <i class="fa fa-code fa-fw"></i> Code
              </a>
              <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#ksd-measure-transport-abs">
                Abstract
              </button>
            </div>
            <div id="ksd-measure-transport-abs" class="collapse abstract">
                Measure transport underpins several recent algorithms for posterior approximation in the Bayesian context, wherein a transport map is sought to minimise the Kullback-Leibler divergence (KLD) from the posterior to the approximation. The KLD is a strong mode of convergence, requiring absolute continuity of measures and placing restrictions on which transport maps can be permitted. Here we propose to minimise a kernel Stein discrepancy (KSD) instead, requiring only that the set of transport maps is dense in an L2 sense and demonstrating how this condition can be validated. The consistency of the associated posterior approximation is established and empirical results suggest that KSD is competitive and more flexible alternative to KLD for measure transport.
            </div>
          </li>
            <li class="publication">
              <h4>
                <small>2017/08</small>
                Continuously tempered Hamiltonian Monte Carlo
              </h4>
              <p>
                <i>Matthew M. Graham</i> and <a href="http://homepages.inf.ed.ac.uk/amos">Amos J. Storkey</a>
              </p>
              <p class='published-in'>
                Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence
              </p>
              <div class="btn-group" role="group">
                <a href="http://auai.org/uai2017/proceedings/papers/289.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-f"></i> Proceedings
                </a>
                <a href="http://auai.org/uai2017/proceedings/supplements/289.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-f"></i> Supplement
                </a>
                <a href="https://arxiv.org/abs/1704.03338" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> arXiv
                </a>
                <a href="https://github.com/matt-graham/continuously-tempered-hmc" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#cthmc-abs">
                  Abstract
                </button>
              </div>
              <div id="cthmc-abs" class="collapse abstract">
                Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC) method for performing approximate inference in complex probabilistic models of continuous variables. In common with many MCMC methods, however, the standard HMC approach performs poorly in distributions with multiple isolated modes. We present a method for augmenting the Hamiltonian system with an extra continuous temperature control variable which allows the dynamic to bridge between sampling a complex target distribution and a simpler unimodal base distribution. This augmentation both helps improve mixing in multimodal targets and allows the normalisation constant of the target distribution to be estimated. The method is simple to implement within existing HMC code, requiring only a standard leapfrog integrator. We demonstrate experimentally that the method is competitive with annealed importance sampling and simulating tempering methods at sampling from challenging multimodal distributions and estimating their normalising constants.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2017/04</small>
                Asymptotically exact inference in differentiable generative models
              </h4>
              <p>
                <i>Matthew M. Graham</i> and <a href="http://homepages.inf.ed.ac.uk/amos">Amos J. Storkey</a>
              </p>
              <p class='published-in'>
                Proceedings of the 20th International Conference on Artificial Intelligence and Statistics
              </p>
              <div class="btn-group" role="group">
                <a href="http://proceedings.mlr.press/v54/graham17a.html" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Proceedings
                </a>
                <a href="https://arxiv.org/abs/1605.07826" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> arXiv
                </a>
                <a href="files/dgm_poster.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Poster
                </a>
                <a href="slides/dgm" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://github.com/matt-graham/differentiable-generative-models" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#dgm-abs">
                  Abstract
                </button>
              </div>
              <div id="dgm-abs" class="collapse abstract">
                Many generative models can be expressed as a differentiable function of random inputs drawn from some simple probability density. This framework includes both deep generative architectures such as Variational Autoencoders and a large class of procedurally defined simulator models. We present a method for performing efficient MCMC inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between inputs exactly consistent with observations. We validate the method by performing inference tasks in a diverse set of models.
              </div>
            </li>
            <li class="publication">
              <h4><small>2016/05</small> Pseudo-Marginal Slice Sampling</h4>
              <p>
                <a href="http://homepages.inf.ed.ac.uk/imurray2/">Iain Murray</a> and <i>Matthew M. Graham</i>
              </p>
              <p class='published-in'>
                Proceedings of the 19th International Conference on Artificial Intelligence and Statistics
              </p>
              <div class="btn-group" role="group">
                <a href="http://proceedings.mlr.press/v51/murray16.html" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Proceedings
                </a>
                <a href="https://arxiv.org/abs/1510.02958" class="btn btn-default">
                  <i class="ai ai-arxiv fa-fw"></i> arXiv
                </a>
                <a href="files/pm_slice_poster.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Poster
                </a>
                <a href="https://github.com/matt-graham/auxiliary-pm-mcmc" class="btn btn-default">
                  <i class="fa fa-code fa-fw"></i> Code
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#pm-slice-abs">
                  Abstract
                </button>
              </div>
              <div id="pm-slice-abs" class="collapse abstract">
                Markov chain Monte Carlo (MCMC) methods asymptotically sample from complex probability distributions.
                The pseudo-marginal MCMC framework only requires an unbiased estimator of the unnormalized probability
                distribution function to construct a Markov chain. However, the resulting chains are harder to tune
                to a target distribution than conventional MCMC, and the types of updates available are limited.
                We describe a general way to clamp and update the random numbers used in a pseudo-marginal method's
                unbiased estimator. In this framework we can use slice sampling and other adaptive methods. We obtain
                more robust Markov chains, which often mix more quickly.
              </div>
            </li>
          </ul>
          <h2 class='sub-heading'>Workshop papers </h2>
          <ul class="publications-list">
            <li class="publication">
              <h4>
                <small>2017/08</small>
                Inference in differentiable generative models
              </h4>
              <p>
                <i>Matthew M. Graham</i> and <a href="http://homepages.inf.ed.ac.uk/amos">Amos J. Storkey</a>
              </p>
              <p class='published-in'>
                ICML 2017 workshop: Implicit generative models
              </p>
              <div class="btn-group" role="group">
                <a href="https://matt-graham.github.io/files/inference-in-differentiable-generative-models-graham-and-storkey.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Extended abstract
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#dgm-wk-abs">
                  Abstract
                </button>
              </div>
              <div id="dgm-wk-abs" class="collapse abstract">
                Many generative models can be expressed as a differentiable function of random inputs drawn from a known probability distribution. This framework includes both learnt parametric generative models and a large class of procedurally defined simulator models. We present a method for performing efficient Markov chain Monte Carlo (MCMC) inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to move between inputs exactly consistent with observations.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2016/12</small>  Continuously tempered Hamiltonian Monte Carlo
              </h4>
              <p>
                <i>Matthew M. Graham</i> and <a href="http://homepages.inf.ed.ac.uk/amos">Amos J. Storkey</a>
              </p>
              <p class='published-in'>
                NIPS 2016 workshop: Advances in Approximate Bayesian Inference
              </p>
              <div class="btn-group" role="group">
                <a href="http://approximateinference.org/accepted/GrahamStorkey2016.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Extended abstract
                </a>
                <a href="files/cthmc/poster.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Poster
                </a>
                <a href="slides/cthmc/aabi" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#cthmc-wk-abs">
                  Abstract
                </button>
              </div>
              <div id="cthmc-wk-abs" class="collapse abstract">
                Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC) method for performing approximate inference in complex probabilistic models of continuous variables. In common with many MCMC methods however the standard HMC approach performs poorly in distributions with multiple isolated modes. Based on an approach proposed in the statistical physics literature, we present a method for augmenting the Hamiltonian system with an extra continuous temperature control variable which allows the dynamic to bridge between sampling a complex target distribution and a simpler uni-modal base distribution. This augmentation both helps increase mode-hopping in multi-modal targets and allows the normalisation constant of the target distribution to be estimated. The method is simple to implement within existing HMC code, requiring only a standard leapfrog integrator. It produces MCMC samples from the target distribution which can be used to directly estimate expectations without any importance re-weighting.
              </div>
            </li>
          </ul>
          <h2 class='sub-heading'> Theses and dissertations </h2>
          <ul class="publications-list">
            <li class="publication">
              <h4>
                <small>2018/07</small>
                Auxiliary variable Markov chain Monte Carlo methods
              </h4>
              <p>
                <i>Matthew M. Graham</i>
              </p>
              <p class='published-in'>
                PhD thesis, University of Edinburgh
              </p>
              <div class="btn-group" role="group">
                <a href="http://hdl.handle.net/1842/28962" class="btn btn-default">
                  <i class="fa fa-file-o fa-fw"></i> Edinburgh Research Archive record
                </a>
                <a href="files/phd_thesis.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Thesis
                </a>
                <a href="slides/viva" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#phd-thesis-abs">
                  Abstract
                </button>
              </div>
              <div id="phd-thesis-abs" class="collapse abstract">
                Markov chain Monte Carlo (MCMC) methods are a widely applicable  class of algorithms for estimating integrals in statistical inference problems.  A common approach in MCMC methods is to introduce additional auxiliary variables into the Markov chain state and perform transitions in the joint space of target and auxiliary variables. In this thesis we consider novel methods for using auxiliary variables within MCMC methods to allow approximate inference in otherwise intractable models and to improve sampling performance in models exhibiting challenging properties such as multimodality. We first consider the pseudo-marginal framework. This extends the Metropolis–Hastings algorithm to cases where we only have access to an unbiased estimator of the density of target distribution. The resulting chains can sometimes show ‘sticking’ behaviour where long series of proposed updates are rejected. Further the algorithms can be difficult to tune and it is not immediately clear how to generalise the approach to alternative transition operators. We show that if the auxiliary variables used in the density estimator are included in the chain state it is possible to use new transition operators such as those based on slice-sampling algorithms within a pseudo-marginal setting. This auxiliary pseudo-marginal approach leads to easier to tune methods and is often able to improve sampling efficiency over existing approaches. As a second contribution we consider inference in probabilistic models defined via a generative process with the probability density of the outputs of this process only implicitly defined. The approximate Bayesian computation (ABC) framework allows inference in such models when conditioning on the values of observed model variables by making the approximation that generated observed variables are ‘close’ rather than exactly equal to observed data. Although making the inference problem more tractable, the approximation error introduced in ABC methods can be difficult to quantify and standard algorithms tend to perform poorly when conditioning on high dimensional observations. This often requires further approximation by reducing the observations to lower dimensional summary statistics. We show how including all of the random variables used in generating model outputs as auxiliary variables in a Markov chain state can allow the use of more efficient and robust MCMC methods such as slice sampling and Hamiltonian Monte Carlo (HMC) within an ABC framework. In some cases this can allow inference when conditioning on the full set of observed values when standard ABC methods require reduction to lower dimensional summaries for tractability. Further we introduce a novel constrained HMC method for performing inference in a restricted class of differentiable generative models which allows conditioning the generated observed variables to be arbitrarily close to observed data while maintaining computational tractability. As a final topicwe consider the use of an auxiliary temperature variable in MCMC methods to improve exploration of multimodal target densities and allow estimation of normalising constants. Existing approaches such as simulated tempering and annealed importance sampling use temperature variables which take on only a discrete set of values. The performance of these methods can be sensitive to the number and spacing of the temperature values used, and the discrete nature of the temperature variable prevents the use of gradient-based methods such as HMC to update the temperature alongside the target variables. We introduce new MCMC methods which instead use a continuous temperature variable. This both removes the need to tune the choice of discrete temperature values and allows the temperature variable to be updated jointly with the target variables within a HMC method.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2013/07</small>
                Insect olfactory landmark navigation
              </h4>
              <p>
                <i>Matthew M. Graham</i>
              </p>
              <p class='published-in'>
                MSc by Research dissertation, University of Edinburgh
              </p>
              <div class="btn-group" role="group">
                <a href="files/msc_project_report.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Dissertation
                </a>
                <a href="files/msc_project_poster.pdf" class="btn btn-default">
                  <i class="fa fa-file-pdf-o fa-fw"></i> Poster
                </a>
                <button class="btn btn-default btn-abs collapsed" data-toggle="collapse" data-target="#msc-thesis-abs">
                  Abstract
                </button>
              </div>
              <div id="msc-thesis-abs" class="collapse abstract">
                The natural world is full of chemical signals - organisms of all scales and taxonomic classifications transmit and receive chemical signals to guide the full gamut of life’s processes: from helping forming mother-infant bonds, to identifying potential mates and even signalling their own deaths. Insects are particularly reliant on chemical cues to guide their behaviour and understanding how insects respond to and use chemical cues in their environment is a high active research area. In a series of recent studies Steck et al. produced evidence that foragers of the Saharan desert ant species Cataglyphis fortis are able to learn an association between an array of odour sources arranged around the entrance to their nest and the relative location of the nest entrance and later use the information they receive from the odour sources to help them navigate to the visually inconspicious nest entrance. This ability to use odour sources as olfactory landmarks had not been previously seen experimentally in insects, and is a remarkable behaviour given the extremely complex and highly dynamic nature of the olfactory signals received by the ants from the turbulent odour plumes the chemicals travel in from the sources. After an introductory chapter covering some relevant background theory to the work in this project, the second chapter of this dissertation will detail a field study conducted with the European desert ant species Cataglyphis velox. As in the studies of Steck et al. the ants were constrained to moving a linear channel and so the navigation task limited to being one-dimensional, the aim of this study was to see if there was any evidence supporting the hypothesis that Cataglyphis velox ants are able to use olfactory landmarks to navigate in a more realistic open environment. The results of the study were inconclusive, due to the low sample sizes that were collected and small effect size in the study design used, however it is proposed that the study could be considered usefully as pilot for a full study at a later date, and an adjusted study design is proposed that might overcome a lot of the issues encountered in the current study. In the third and final chapter of this dissertation, a modelling study of what information is available in the olfactory signal received from a turbulent odour plume about the location of the source of that plume is presented, with this work aiming to explore the information which may being used by Cataglyphis desert ants when using olfactory landmarks to navigate. The details of the plume and olfactory sensor models used are described and the results of an analysis of the estimated mutual information between the modelled olfactory signals and the location of odour source presented. It is found that the locational informational content of individual signal segment statistics seems to be low, though combining multiple statistics does potentially allow more useful reductions in uncertainty.
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2012/06</small>
                Measuring tissue stiffness with ultrasound
              </h4>
              <p>
                <i>Matthew M. Graham</i>
              </p>
              <p class='published-in'>
                MEng project report, University of Cambridge
              </p>
                <div class="btn-group" role="group">
                  <a href="files/meng_project_report.pdf" class="btn btn-default">
                    <i class="fa fa-file-pdf-o fa-fw"></i> Report
                  </a>
                  <a href="files/meng_project_presentation.pdf" class="btn btn-default">
                    <i class="fa fa-file-pdf-o fa-fw"></i> Presentation
                  </a>
                  <a href="files/meng_project_abstract.pdf" class="btn btn-default">
                    <i class="fa fa-file-pdf-o fa-fw"></i> Abstract
                  </a>
                </div>
          </ul>
        </div> <!-- row Publications -->
      </div>
    </div>

    <div class="row" id="talks">
      <div class="col-xs-12">
        <div class="row section-header">
          <h1> Talks </h1>
        </div>
        <div class="row">
          <ul class="publications-list">
            <li class="publication">
              <h4>
                <small>2022/04</small> A brief introduction to automatic differentiation
              </h4>
              <p class='published-in'>
                Monte Carlo reading group @ UCL Statistical Science
              </p>
              <div class="btn-group" role="group">
                <a href="slides/ad/index.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2022/03</small> Lift and flow: manifold MCMC methods for efficient inference in stiff posteriors
              </h4>
              <p class='published-in'>
                <a href='https://isds-department.essec.edu/research/research-seminars/data-analytics'>ESSEC Econometrics and Statistics Seminar</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/manifold-lifting/essec.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2021/05</small> Lift and flow: manifold MCMC methods for efficient inference in stiff posteriors
              </h4>
              <p class='published-in'>
                <a href='https://www.ucl.ac.uk/statistics/seminar'>UCL Statistical Science Seminar</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/manifold-lifting/ucl.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://www.youtube.com/watch?v=9o_ZrfKHpJE" class="btn btn-default">
                  <i class="fa fa-file-video-o fa-fw"></i> Video
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2020/12</small> MCMC methods for inverse problems with highly informative observations
              </h4>
              <p class='published-in'>
                <a href='http://www.cmstatistics.org/RegistrationsV2/CMStatistics2020/viewSubmission.php?in=1125&token=7021os60479s43o00nsqr2p918r9r87n'>CMStatistics 2020</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/manifold-lifting/cmstatistics.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2020/08</small> Manifold MCMC methods for inference in diffusion models
              </h4>
              <p class='published-in'>
                <a href='https://mcqmc20.web.ox.ac.uk/'>MCQMC 2020</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/sde/index.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://www.youtube.com/watch?v=tJNUqWlvhc0" class="btn btn-default">
                  <i class="fa fa-file-video-o fa-fw"></i> Video
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2020/04</small> Manifold lifting: scaling MCMC methods to the vanishing noise limit
              </h4>
              <div class="btn-group" role="group">
                <a href="slides/manifold-lifting/index.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2019/03</small> Sidestepping intractability by augmentation: auxiliary variable pseudo-marginal methods
              </h4>
              <div class="btn-group" role="group">
                <a href="slides/auxvar/apm.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2019/03</small> Spatially smooth local ensemble transport particle filtering
              </h4>
              <p class='published-in'>
                <a href='https://www.stat.nus.edu.sg/index.php/events/academic/seminars/eventdetail/90/-/spatially-smooth-local-ensemble-transform-particle-filtering'>Seminar at Department of Statistics and Applied Probability, National University of Singapore.</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/sletpf/nus.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2019/01</small> A brief introduction to automatic differentation
              </h4>
              <div class="btn-group" role="group">
                <a href="slides/ad/index.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2018/03</small> Asymptotically exact inference in differentiable generative models
              </h4>
              <p class='published-in'>
                <a href='https://www.maths.nottingham.ac.uk/personal/tk/bayescomp/'>BayesComp 2018, Barcelona</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/dgm/bayescomp.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
              <li class="publication">
                <h4>
                  <small>2017/11</small> Inference in differentiable generative models
                </h4>
                <p class='published-in'>
                  <a href='https://www.stat.nus.edu.sg/index.php/events/academic/seminars/eventdetail/28/-/inference-in-differentiable-generative-models'>Seminar at Department of Statistics and Applied Probability, National University of Singapore.</a>
                </p>
                <div class="btn-group" role="group">
                  <a href="slides/dgm/nus.html" class="btn btn-default">
                    <i class="fa fa-television fa-fw"></i> Slides
                  </a>
                </div>
              </li>
            <li class="publication">
              <h4>
                <small>2017/08</small> Continuously tempered Hamiltonian Monte Carlo
              </h4>
              <p class='published-in'>
                <a href='http://auai.org/uai2017/schedule.php'>33rd Conference on Uncertainty in Artificial Intelligence</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/cthmc/uai.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2017/07</small> Inference in implicit generative models
              </h4>
              <p class='published-in'>
                Seminar at School of Mathematics and Statistics, University of Newcastle
              </p>
              <div class="btn-group" role="group">
                <a href="slides/dgm/newcastle.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2017/04</small> Asymptotically exact inference in differentiable generative models
              </h4>
              <p class='published-in'>
                <a href='http://www.aistats.org'>20th International Conference on Artificial Intelligence and Statistics</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/dgm/aistats.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2017/02</small> Inference in differentiable generative models
              </h4>
              <p class='published-in'>
                <a href='https://www.birs.ca/events/2017/5-day-workshops/17w5025'>BIRS workshop: Validating and Expanding Approximate Bayesian Computation Methods</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/birs" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://www.birs.ca/events/2017/5-day-workshops/17w5025/videos/watch/201702201643-Graham.html" class="btn btn-default">
                  <i class="fa fa-file-video-o fa-fw"></i> Video
                </a>
              </div>
            </li>
            <li class="publication">
              <h4>
                <small>2016/12</small> Continuously tempered Hamiltonian Monte Carlo
              </h4>
              <p class='published-in'>
                <a href='http://approximateinference.org/'>NIPS 2016 workshop: Advances in Approximate Bayesian Inference</a>
              </p>
              <div class="btn-group" role="group">
                <a href="slides/cthmc/aabi.html" class="btn btn-default">
                  <i class="fa fa-television fa-fw"></i> Slides
                </a>
                <a href="https://www.youtube.com/watch?v=4fi-lZ8mn5Q&index=11&list=PL8Yb49e5zFuztzY4wZRp_XIj6PREg3pw8" class="btn btn-default">
                  <i class="fa fa-youtube fa-fw"></i> Video
                </a>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <div class="row" id="footer">
      <div class="col-xs-12">
        <hr />
        <p class="text-center">
          Built using <a href="http://getbootstrap.com">Bootstrap</a>. Icons from <a href="http://fortawesome.github.io/Font-Awesome/">FontAwesome</a> and <a href="https://jpswalsh.github.io/academicons/">Academicons</a>.
        </p>
      </div>
    </div><!--./footer-->

  </div><!-- /.container -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
  <script src="js/bootstrap.js"></script>
  <script src="js/plugins.js"></script>

  <script>
    $('.github-card').each(function( i, div ) {
      var request = new XMLHttpRequest();
      request.onload = function () {
        var repo = JSON.parse(this.responseText);
        div.innerHTML = `<div class="panel-heading"><i class="fa fa-github fa-fw"></i> <a href="${repo.html_url}">${repo.name}</a> <small>(${repo.language})</small></div><div class="panel-body">${repo.description}</div>`;
      };
      request.open('get', 'https://api.github.com/repos/'.concat( div.dataset.repo), true);
      request.send();
    });
  </script>


</body>

</html>
